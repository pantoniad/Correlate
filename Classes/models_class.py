import numpy as np

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error
from sklearn.model_selection import learning_curve, LearningCurveDisplay

from typing import Optional
from Classes.latex_class import latex as ltx

import matplotlib.pyplot as plt


class models_per_OP:

    def __init__(self, data: pd.DataFrame, features: list, response: list):
        
        self.data = data
        self.features = features
        self.response = response

    def splitter(self, train_split: Optional[float] = 0.6, test_split: Optional[float] = 0.4, dev_split: Optional[float] = 0.2):

        """
        splitter: Splits the data into train, dev and test based on the proportions given above. 
        If dev test is not needed, the user can define only the splits of the train and test 
        sets. 

        Inputs:
        - self
        - train_split: the percentage of data used for model training, float,
        - test_split: the percentage of data used for testing, float,
        - dev_split: the percentage of data used for the development, float

        Outputs:
        - xtrain, ytrain: data part for the training
        - xdev, ydev: data part for the development
        - xtest, ytest: data part for the testing

        """
        # Extract data from self
        data = self.data
        x = self.features
        y = self.response

        # Split data: Train and temp
        xtrain, Xtemp, ytrain, Ytemp = train_test_split(
            x, y, train_size=train_split, random_state=42
        )

        # Split data: Temp to Dev and Test
        # Dev: train, Test: test, get split %
        size = dev_split/(test_split+dev_split)

        xdev, xtest, ydev, ytest = train_test_split(
            Xtemp, Ytemp, train_size=size, random_state=42
        )

        return xtrain, ytrain, xdev, ydev, xtest, ytest

    def polReg(self, xtrain: pd.DataFrame, ytrain: pd.DataFrame, xtest: pd.DataFrame, ytest: pd.DataFrame,
               parameters: dict):

        """
        polReg: Polynomial Regression model. This function houses the code
        for generating a polynomial regression model

        Inputs: 
        - xtrain, ytrain: the features and response data used for 
        trainning, Dataframe
        - xtest, ytest: the features and response data used for 
        testing, Dataframe
        -parameters: the parameters needed to define the polynomial 
        regression model, Dictionary

        Outputs:
        - lin: the linear regression model fitted to the data,
        - train_results: the response data used for training and the
        predicted values generated by the model used, Dataframe
        - test_results: the response data used for testing and the 
        predicted values generated by the model used, Dataframe

        """
        
        # Upack from self
        data = self.data

        # Extract parameters
        deg = parameters["Degrees"]
        bias = parameters["Include Bias"]

        x_scaler = StandardScaler()
        xtrain_scaled = x_scaler.fit_transform(xtrain)
        xtest_scaled  = x_scaler.transform(xtest)

        # Polynomial object
        poly = PolynomialFeatures(degree = deg, include_bias = bias)
        x_train_poly = poly.fit_transform(xtrain_scaled)
        x_test_poly = poly.transform(xtest_scaled)
        
        # Linear regression objet
        lin = LinearRegression()
        lin.fit(x_train_poly, ytrain)

        # Predict
        y_train_pred = lin.predict(x_train_poly)
        y_test_pred = lin.predict(x_test_poly)
        
        # Create output dataframe
        d1 = {
           "Y train": ytrain,
           "Y train Pred": y_train_pred,
        }
        
        d2 = {
           "Y test": ytest,
           "Y test Pred": y_test_pred,
        }
        
        train_results = pd.DataFrame(data = d1)
        test_results = pd.DataFrame(data = d2)

        return lin, poly, train_results, test_results
    
    def performance_metrics(self, train: pd.DataFrame, test: pd.DataFrame):
                           # to_latex: bool = False, parameters = Optional[pd.DataFrame]):
        """
        performance_metrics: This function returns the values of a series of performance metrics
        The performance metrics implemented are:
        - Mean Absolute Error,
        - Root Mean Absolute Error,
        - R squared

        Inputs:
        - self,
        - train: Dataframe, Contains the response of the training dataset and the predicted 
        response from the fitted model,
        - test: Dataframe, Contains the response of the test set and the predicted response 
        from the fitted model

        Outputs:
        - metrics: Dataframe, Contains the values of the metrics. 
        Keys: metrics, Index: Train, Test

        Additional features: 
        - Convert the metrics dataframe into a latex table using the corresponding class

        """
        # Extract valuers from dataframe
        ytrain = train["Y train"]
        y_train_pred = train["Y train Pred"]
        ytest = test["Y test"]
        y_test_pred = test["Y test Pred"]

        # Evaluate results
        train_mse = mean_absolute_error(ytrain, y_train_pred)
        test_mse = mean_absolute_error(ytest, y_test_pred)
        train_rmse = root_mean_squared_error(ytrain, y_train_pred)
        test_rmse = root_mean_squared_error(ytest, y_test_pred)
        train_r2 = r2_score(ytrain, y_train_pred)
        test_r2 = r2_score(ytest, y_test_pred)

        # Results to dataframe
        d = {
           "MSE":{
               "Train": train_mse,
               "Test": test_mse
           },
           "RMSE":{
               "Train": train_rmse,
               "Test": test_rmse
           },
           "R2":{
               "Train": train_r2,
               "Test": test_r2
           }
        } 
        
        metrics = pd.DataFrame(data = d)

        """
        # Save to latex table
        path = parameters["Path"]
        title = parameters["Title"]
        caption = parameters["Caption"]
        label = parameters["Label"]
        headers =  metrics.keys()

        if to_latex == True:
            ltx1 = ltx(df = d, filename = path, caption = caption, label = label, header  = headers)
            ltx1.df_to_lxTable()
        else:
            pass
        """
        return metrics
    
    def Learning_curve(self, model: object, model_features: Optional[object] = None, operating_point: Optional[str] = None):

        """
        Learning_curve: creates a learning curve for the given data set, 

        Inputs:
        - model: the fitted model, object
        - model_features: Optional entry that contains the features of the model. 
        i.e. if Polynomial regression is used, then the Polynomial Features method 
        has to be imported to get the characteristics of the polynomial fit,
        - operating_point: the operating point that the engine works at.
        Used for plotting purposes, str
        """

        # Extract data from self
        data = self.data

        # Break the data down into based on the operating point
        X = data.iloc[:, range(0, len(data.keys())-1)]
        y = data.iloc[:, -1]

        x_scaler = StandardScaler()
        X_scaled = x_scaler.fit_transform(X)
        #y_scaled  = x_scaler.transform(y)

        # Learning curve 
        train_sizes, train_scores, test_scores = learning_curve(model, X_scaled, y, train_sizes = np.arange(0.1, 1, 0.05), cv = 7, scoring = "r2")
        
        # Mean and std values
        mean_train = np.mean(train_scores, axis = 1)
        mean_test = np.mean(test_scores, axis = 1)
        std_train = np.std(train_scores, axis = 1)
        std_test = np.std(test_scores, axis = 1)

        # Extract model features
        if model_features.degree != None:
            model_type = f"Pol. Regresion ({model_features.degree})"
        else:
            pass

        # Visualize No.1
        fig1, ax1 = plt.subplots( figsize = (7, 5))
        plt.plot(train_sizes, mean_train, "--o", color = "blue", label = "Train R2")
        plt.plot(train_sizes, mean_test, "-o", color = "red", label = "Test R2")
        plt.plot()

        ax1.fill_between(train_sizes, mean_train + std_train, mean_train - std_train,
                         where=((mean_train + std_train) >= (mean_train - std_train)),
                         color = "royalblue", alpha = 0.2)
        
        ax1.fill_between(train_sizes, mean_test + std_test, mean_test - std_test,
                         where=((mean_test + std_test) >= (mean_test - std_test)),
                         color = "red", alpha = 0.2)



        plt.xlabel("Training set size")
        plt.ylabel("Score - Coefficient of Determination (R2)")
        plt.grid(color = "silver", linestyle = ":")
        #plt.ylim([min(min(mean_test), min(mean_train)) - 0.01*min(min(mean_test), min(mean_train)),
        #           max(max(mean_test), max(mean_train)) + 0.01*max(max(mean_test), max(mean_train))])
        
        if operating_point != None: 
            plt.title(f"Learning curve - {operating_point} conditions - {model_type}")
        else:
            plt.title(f"Learning curve - {model_type}")
        
        plt.legend()
        plt.show()

    def iterate_through_ops(self,):

        """
        iterate_through_ops:

        Inputs:

        Outputs:

        """

        pass

