import numpy as np

import pandas as pd
import sklearn
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_percentage_error, r2_score, root_mean_squared_error
from sklearn.model_selection import learning_curve

from typing import Optional
from Classes.latex_class import latex as ltx

import matplotlib.pyplot as plt

import torch
import torch.nn.functional as F
import torch.nn as nn

np.random.seed(42)

class models_per_OP:

    def __init__(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):
        
        """
        Inputs:
        - X_train:
        - y_train:
        - X_test:
        - y_test:
        """
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test

    def polReg(self, parameters: dict):

        """
        polReg: Polynomial Regression model. This function houses the code
        for generating a polynomial regression model

        Inputs: 
        -parameters: the parameters needed to define the polynomial 
        regression model, Dictionary

        Outputs:
        - lin: the linear regression model fitted to the data,
        - train_results: the response data used for training and the
        predicted values generated by the model used, Dataframe
        - test_results: the response data used for testing and the 
        predicted values generated by the model used, Dataframe

        """
        
        # Upack from self
        X_train = self.X_train
        X_test = self.X_test
        y_train = self.y_train
        y_test = self.y_test

        # Extract parameters
        deg = parameters["Degrees"]
        bias = parameters["Include Bias"]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled  = scaler.transform(X_test)

        # Polynomial object
        poly = PolynomialFeatures(degree = deg, include_bias = bias)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        
        # Linear regression objet
        lin = LinearRegression()
        lin.fit(X_train_poly, y_train)

        # Predict
        y_train_pred = lin.predict(X_train_poly)
        y_test_pred = lin.predict(X_test_poly)
        
        # Create output dataframe
        d1 = {
           "Y train": y_train,
           "Y train Pred": y_train_pred,
        }
        
        d2 = {
           "Y test": y_test,
           "Y test Pred": y_test_pred,
        }
        
        # Get returns ready
        train_results = pd.DataFrame(data = d1)
        test_results = pd.DataFrame(data = d2)
        model_features = {
            "Model type": "Polynomial Regression",
            "Model features": poly
        }

        return lin, model_features, scaler, train_results, test_results

    def gradientBoosting(self, parameters: dict = None):
        """
        gbr: GradientBoostingRegressor wrapper

        Inputs:
        -

        Outputs:
        -
        
        """
        # Extract parameters from self
        X_train = self.X_train
        X_test = self.X_test
        y_train = self.y_train
        y_test = self.y_test
        
        # Scale data
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled  = scaler.transform(X_test) 

        # Initialiaze regressor
        gbr = GradientBoostingRegressor()

        # Train Regressor 
        gbr.fit(X_train_scaled, y_train)

        # Predict based on test
        y_train_pred = gbr.predict(X_train_scaled)
        y_test_pred = gbr.predict(X_test_scaled)

        # Create output dataframes
        d1 = {
           "Y train": y_train,
           "Y train Pred": y_train_pred,
        }
        
        d2 = {
           "Y test": y_test,
           "Y test Pred": y_test_pred,
        }
        
        # Get returns ready
        train_results = pd.DataFrame(data = d1)
        test_results = pd.DataFrame(data = d2)
        model_features = {
            "Model type": "Gradient Boosting"
        }

        return gbr, model_features, scaler, train_results, test_results


    def performance_metrics(self, train: pd.DataFrame, test: pd.DataFrame):
                           # to_latex: bool = False, parameters = Optional[pd.DataFrame]):
        """
        performance_metrics: This function returns the values of a series of performance metrics
        The performance metrics implemented are:
        - Mean Absolute Error,
        - Root Mean Absolute Error,
        - R squared

        Inputs:
        - self,
        - train: Dataframe, Contains the response of the training dataset and the predicted 
        response from the fitted model,
        - test: Dataframe, Contains the response of the test set and the predicted response 
        from the fitted model

        Outputs:
        - metrics: Dataframe, Contains the values of the metrics. 
        Keys: metrics, Index: Train, Test

        Additional features: 
        - Convert the metrics dataframe into a latex table using the corresponding class

        """
        # Extract valuers from dataframe
        y_train = train["Y train"]
        y_train_pred = train["Y train Pred"]
        y_test = test["Y test"]
        y_test_pred = test["Y test Pred"]

        # Evaluate results
        train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
        test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
        train_rmse = root_mean_squared_error(y_train, y_train_pred)
        test_rmse = root_mean_squared_error(y_test, y_test_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)

        # CRMSD
        pred = y_test_pred.values.astype(float)
        pred_median = np.median(y_test_pred.values.astype(float))
        real = y_test.values.astype(float) 
        real_median = np.median(y_test.values.astype(float))
        crmsd_test = np.sqrt(1/len(pred)*np.sum(((pred - pred_median)-(real - real_median))**2))

        # Results to dataframe
        d = {
           "MAPE":{
               "Train": train_mape,
               "Test": test_mape
           },
           "RMSE":{
               "Train": train_rmse,
               "Test": test_rmse
           },
           "R2":{
               "Train": train_r2,
               "Test": test_r2
           },
           "CRMSD":{
               "Train": "-",
               "Test": crmsd_test
           }
        } 
        
        metrics = pd.DataFrame(data = d)

        """
        # Save to latex table
        path = parameters["Path"]
        title = parameters["Title"]
        caption = parameters["Caption"]
        label = parameters["Label"]
        headers =  metrics.keys()

        if to_latex == True:
            ltx1 = ltx(df = d, filename = path, caption = caption, label = label, header  = headers)
            ltx1.df_to_lxTable()
        else:
            pass
        """
        return metrics
    
    def Learning_curve(self, data: pd.DataFrame, scaler: sklearn.preprocessing.StandardScaler, model: object, model_features: Optional[object] = None, operating_point: Optional[str] = None):

        """
        Learning_curve: creates a learning curve for the given data set, 

        Inputs:
        - model: the fitted model, object
        - model_features: Only applicable to PolynomialFeatures. Optional entry 
        that contains the features of the model.  
        - operating_point: the operating point that the engine works at.
        Used for plotting purposes, str
        """

        # Break the data down
        X = data.iloc[:, range(0, len(data.keys())-1)]
        y = data.iloc[:, -1]

        X_scaled = scaler.fit_transform(X)

        # Learning curve 
        train_sizes, train_scores, test_scores = learning_curve(model, X_scaled, y, train_sizes = np.arange(0.1, 1, 0.05), cv = 7, scoring = "r2")
        
        # Mean and std values
        mean_train = np.mean(train_scores, axis = 1)
        mean_test = np.mean(test_scores, axis = 1)
        std_train = np.std(train_scores, axis = 1)
        std_test = np.std(test_scores, axis = 1)

        # Extract model features
        if model_features["Model type"] == "Polynomial Regression":
            model_type = f"{model_features["Model type"]}: ({model_features["Model features"].degree})"
        elif model_features["Model type"] == "Gradient Boosting":
            model_type = f"{model_features["Model type"]}"
        elif model_features["Model type"] == "Artificial Neural Networks":
            model_type = f"{model_features["Model type"]}"

        # Visualize No.1
        fig1, ax1 = plt.subplots( figsize = (7, 5))
        plt.plot(train_sizes, mean_train, "--o", color = "blue", label = "Train R2")
        plt.plot(train_sizes, mean_test, "-o", color = "red", label = "Test R2")
        plt.plot()

        ax1.fill_between(train_sizes, mean_train + std_train, mean_train - std_train,
                         where=((mean_train + std_train) >= (mean_train - std_train)),
                         color = "royalblue", alpha = 0.2)
        
        ax1.fill_between(train_sizes, mean_test + std_test, mean_test - std_test,
                         where=((mean_test + std_test) >= (mean_test - std_test)),
                         color = "red", alpha = 0.2)



        plt.xlabel("Training set size")
        plt.ylabel("Score - Coefficient of Determination (R2)")
        plt.grid(color = "silver", linestyle = ":")
        
        if operating_point != None: 
            plt.title(f"Learning curve - {operating_point} conditions - {model_type}")
        else:
            plt.title(f"Learning curve - {model_type}")
        
        plt.legend()
        plt.show()

    #def Learning_curve_ann(self, ):

    class ann():

        class Model(nn.Module):
            """
            
            """

            def __init__(self, in_features: int = 3, h1: int = 5, h2: int = 15, h3: int = 10, h4: int = 5, out_features: int = 1):

                super().__init__() # Intiantiate the nn module

                # Define the structure: In -> Layer 1 -> Layer 2 -> Out using Fully Connected layers (FC)
                self.fc1 = nn.Linear(in_features, h1)
                self.fc2 = nn.Linear(h1, h2)
                self.fc3 = nn.Linear(h2,h3)
                self.fc4 = nn.Linear(h3, h4)
                self.out = nn.Linear(h4, out_features)
                self.float()
                
            def forward(self, x):

                x = F.relu(self.fc1(x)) # relu: Rectified Linear Unit (outputs the input if positive, else outputs zero)
                x = F.relu(self.fc2(x))
                x = F.relu(self.fc3(x))
                x = F.relu(self.fc4(x))
                x = self.out(x)

                return x

        class CustomDataset(torch.utils.data.Dataset):

            def __init__(self, data: pd.DataFrame):

                """
                __init__

                Inputs: 
                - data: pd.Dataframe 
                """
                feature1 = data["Pressure Ratio"]
                feature2 = data["Rated Thrust (kN)"]
                feature3 = data.loc[:, data.columns.str.contains("Fuel Flow")]
                self.features = pd.concat([feature1, feature2, feature3], axis = 1)

                response = data.loc[:, data.columns.str.contains("NOx EI")]
                self.response = response

            def __len__(self):
                return len(self.features)

            def __getitem__(self, index):
                features_sample = self.features.iloc[index,:].values
                response_sample = self.response.iloc[index, :].values
                return torch.tensor(features_sample.astype(np.float32())), torch.tensor(response_sample.astype(np.float32()))

        @staticmethod
        def train_one_epoch(model: torch.nn.Module, optimizer: torch.optim, criterion: torch.nn, train_loader: torch.utils.data.DataLoader, device: str):
             
            """
            train_one_epoch:

            Inputs:
            - model: the model instance, torch.nn.Module,
            - optimizer: the optimizer instance, torch.nn.optim, 
            - criterion: the criterion instance, torch.nn,
            - train_loader: the traininng data in the form of a dataloader, torch.utils.data.DataLoader,
            - device: the device which will be used for training, str

            Outputs:
            - losses_train: list with all 
            
            """

            running_loss = 0

            for j, (features_sample, response_sample) in enumerate(train_loader):

                # Move tensors to device
                features_sample = features_sample.to(device)
                response_sample = response_sample.to(device)

                # Train model
                optimizer.zero_grad()
                y_pred = model.forward(features_sample)
                
                # Get result from loss function
                loss = torch.sqrt(criterion(y_pred, response_sample))
                running_loss += loss

                # Update weights and optimizer
                loss.backward()
                optimizer.step()
            
            avg_loss = running_loss/(j+1)
            return avg_loss

        @staticmethod
        def validate_one_epoch(model: torch.nn.Module, optimizer: torch.optim, criterion: torch.nn, test_loader: torch.utils.data.DataLoader, device: str):
            """
            validate_one_epoch:

            Inputs:

            Outputs:

            """

            running_loss = 0
            with torch.no_grad():
                for j, (features_sample, response_sample) in enumerate(test_loader):
                    
                    # Trasfer tensors to device
                    features_sample = features_sample.to(device)
                    response_sample = response_sample.to(device)

                    # Predict based on the trained model
                    y_pred_v = model(features_sample)
                    loss_v = torch.sqrt(criterion(y_pred_v, response_sample))
                    running_loss += loss_v
                
                avg_loss_v = running_loss /(j+1)

            return avg_loss_v 
