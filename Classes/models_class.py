import numpy as np

import pandas as pd
import sklearn
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_percentage_error, r2_score, root_mean_squared_error
from sklearn.model_selection import learning_curve

from typing import Optional
from Classes.latex_class import latex as ltx

import matplotlib.pyplot as plt

np.random.seed(42)

class models_per_OP:

    def __init__(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):
        
        """
        Inputs:
        - X_train:
        - y_train:
        - X_test:
        - y_test:
        """
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test

    def polReg(self, parameters: dict):

        """
        polReg: Polynomial Regression model. This function houses the code
        for generating a polynomial regression model

        Inputs: 
        -parameters: the parameters needed to define the polynomial 
        regression model, Dictionary

        Outputs:
        - lin: the linear regression model fitted to the data,
        - train_results: the response data used for training and the
        predicted values generated by the model used, Dataframe
        - test_results: the response data used for testing and the 
        predicted values generated by the model used, Dataframe

        """
        
        # Upack from self
        X_train = self.X_train
        X_test = self.X_test
        y_train = self.y_train
        y_test = self.y_test

        # Extract parameters
        deg = parameters["Degrees"]
        bias = parameters["Include Bias"]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled  = scaler.transform(X_test)

        # Polynomial object
        poly = PolynomialFeatures(degree = deg, include_bias = bias)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        
        # Linear regression objet
        lin = LinearRegression()
        lin.fit(X_train_poly, y_train)

        # Predict
        y_train_pred = lin.predict(X_train_poly)
        y_test_pred = lin.predict(X_test_poly)
        
        # Create output dataframe
        d1 = {
           "Y train": y_train,
           "Y train Pred": y_train_pred,
        }
        
        d2 = {
           "Y test": y_test,
           "Y test Pred": y_test_pred,
        }
        
        train_results = pd.DataFrame(data = d1)
        test_results = pd.DataFrame(data = d2)

        return lin, poly, scaler, train_results, test_results

    def gradientBoosting(self, xtrain: pd.DataFrame, xtest: pd.DataFrame, ytrain: pd.DataFrame, ytest: pd.DataFrame, params: dict = None):
        """
        gbr: GradientBoostingRegressor wrapper

        Inputs:
        -

        Outputs:
        -
        
        """
        # Extract parameters from self
        features = self.features
        response = self.response
        
        # Scale data
        x_scaler = StandardScaler()
        xtrain_scaled = x_scaler.fit_transform(xtrain)
        xtest_scaled  = x_scaler.transform(xtest) 

        # Initialiaze regressor
        gbr = GradientBoostingRegressor()

        # Train Regressor 
        gbr.fit(xtrain_scaled, ytrain)

        # Predict based on test
        y_train_pred = gbr.predict(xtrain_scaled)
        y_test_pred = gbr.predict(xtest_scaled)

        # Create output dataframes
        d1 = {
           "Y train": ytrain,
           "Y train Pred": y_train_pred,
        }
        
        d2 = {
           "Y test": ytest,
           "Y test Pred": y_test_pred,
        }
        
        train_results = pd.DataFrame(data = d1)
        test_results = pd.DataFrame(data = d2)

        return gbr, x_scaler, train_results, test_results


    def performance_metrics(self, train: pd.DataFrame, test: pd.DataFrame):
                           # to_latex: bool = False, parameters = Optional[pd.DataFrame]):
        """
        performance_metrics: This function returns the values of a series of performance metrics
        The performance metrics implemented are:
        - Mean Absolute Error,
        - Root Mean Absolute Error,
        - R squared

        Inputs:
        - self,
        - train: Dataframe, Contains the response of the training dataset and the predicted 
        response from the fitted model,
        - test: Dataframe, Contains the response of the test set and the predicted response 
        from the fitted model

        Outputs:
        - metrics: Dataframe, Contains the values of the metrics. 
        Keys: metrics, Index: Train, Test

        Additional features: 
        - Convert the metrics dataframe into a latex table using the corresponding class

        """
        # Extract valuers from dataframe
        y_train = train["Y train"]
        y_train_pred = train["Y train Pred"]
        y_test = test["Y test"]
        y_test_pred = test["Y test Pred"]

        # Evaluate results
        train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
        test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
        train_rmse = root_mean_squared_error(y_train, y_train_pred)
        test_rmse = root_mean_squared_error(y_test, y_test_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)

        # CRMSD
        pred = y_test_pred.values.astype(float)
        pred_median = np.median(y_test_pred.values.astype(float))
        real = y_test.values.astype(float) 
        real_median = np.median(y_test.values.astype(float))
        crmsd_test = np.sqrt(1/len(pred)*np.sum(((pred - pred_median)-(real - real_median))**2))

        # Results to dataframe
        d = {
           "MAPE":{
               "Train": train_mape,
               "Test": test_mape
           },
           "RMSE":{
               "Train": train_rmse,
               "Test": test_rmse
           },
           "R2":{
               "Train": train_r2,
               "Test": test_r2
           },
           "CRMSD":{
               "Train": "-",
               "Test": crmsd_test
           }
        } 
        
        metrics = pd.DataFrame(data = d)

        """
        # Save to latex table
        path = parameters["Path"]
        title = parameters["Title"]
        caption = parameters["Caption"]
        label = parameters["Label"]
        headers =  metrics.keys()

        if to_latex == True:
            ltx1 = ltx(df = d, filename = path, caption = caption, label = label, header  = headers)
            ltx1.df_to_lxTable()
        else:
            pass
        """
        return metrics
    
    def Learning_curve(self, data: pd.DataFrame, scaler: sklearn.preprocessing.StandardScaler, model: object, model_features: Optional[sklearn.preprocessing.PolynomialFeatures] = None, operating_point: Optional[str] = None):

        """
        Learning_curve: creates a learning curve for the given data set, 

        Inputs:
        - model: the fitted model, object
        - model_features: Only applicable to PolynomialFeatures. Optional entry 
        that contains the features of the model.  
        - operating_point: the operating point that the engine works at.
        Used for plotting purposes, str
        """

        # Break the data down into based on the operating point
        X = data.iloc[:, range(0, len(data.keys())-1)]
        y = data.iloc[:, -1]

        X_scaled = scaler.fit_transform(X)

        # Learning curve 
        train_sizes, train_scores, test_scores = learning_curve(model, X_scaled, y, train_sizes = np.arange(0.1, 1, 0.05), cv = 5, scoring = "r2")
        
        # Mean and std values
        mean_train = np.mean(train_scores, axis = 1)
        mean_test = np.mean(test_scores, axis = 1)
        std_train = np.std(train_scores, axis = 1)
        std_test = np.std(test_scores, axis = 1)

        # Extract model features
        if model_features != None:
            model_type = f"Pol. Regresion ({model_features.degree})"

        # Visualize No.1
        fig1, ax1 = plt.subplots( figsize = (7, 5))
        plt.plot(train_sizes, mean_train, "--o", color = "blue", label = "Train R2")
        plt.plot(train_sizes, mean_test, "-o", color = "red", label = "Test R2")
        plt.plot()

        ax1.fill_between(train_sizes, mean_train + std_train, mean_train - std_train,
                         where=((mean_train + std_train) >= (mean_train - std_train)),
                         color = "royalblue", alpha = 0.2)
        
        ax1.fill_between(train_sizes, mean_test + std_test, mean_test - std_test,
                         where=((mean_test + std_test) >= (mean_test - std_test)),
                         color = "red", alpha = 0.2)



        plt.xlabel("Training set size")
        plt.ylabel("Score - Coefficient of Determination (R2)")
        plt.grid(color = "silver", linestyle = ":")
        #plt.ylim([min(min(mean_test), min(mean_train)) - 0.01*min(min(mean_test), min(mean_train)),
        #           max(max(mean_test), max(mean_train)) + 0.01*max(max(mean_test), max(mean_train))])
        
        if operating_point != None: 
            plt.title(f"Learning curve - {operating_point} conditions - {model_type}")
        else:
            plt.title(f"Learning curve - {model_type}")
        
        plt.legend()
        plt.show()